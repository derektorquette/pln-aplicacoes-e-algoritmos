{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derektorquette/pln-com-deep-learning-ia-expert/blob/main/perguntas_respostas_com_BART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0"
      },
      "source": [
        "# Etapa 1: Instalação e importação das bibliotecas\n",
        "\n",
        "- SQUAD Link: https://rajpurkar.github.io/SQuAD-explorer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT9DBs7drn6D"
      },
      "source": [
        "#!pip install tf-models-nightly==2.9.0.dev20220528\n",
        "#!pip install tf-nightly==2.9.0.dev20220201"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-models-official==2.2\n",
        "# !pip install tensorflow==2.2\n",
        "!pip install tensorflow==2.8.0rc0 # atualização\n",
        "!pip install tensorflow==2.16"
      ],
      "metadata": {
        "id": "rNV6tr2bU7Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvsG3TaesS6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d77285d-1ed9-4f75-a51e-26ab90afe1d6"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvgoC31Msa5J"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "#from official.nlp.bert.data.squad_lib import generate_tf_record_from_json_file\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "from official.nlp import optimization\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZuUKOhRtI_J"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5"
      },
      "source": [
        "# Etapa 2: Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JkKHNnoubd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d5c5ed-e36c-4330-f2d3-da2e4dea3ae3"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5YAnEeuulUU"
      },
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/Cursos - recursos/BERT/train-v1.1.json\",\n",
        "    \"/content/drive/My Drive/Cursos - recursos/BERT/vocab.txt\",\n",
        "    \"/content/drive/My Drive/Cursos - recursos/BERT/train-v1.1.tf_record\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_1HyuKv-gO"
      },
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/Cursos - recursos/BERT/train_meta_data\", \"w\") as writer:\n",
        "  writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck5pC9Oowe44"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Cursos - recursos/BERT/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs"
      },
      "source": [
        "# Etapa 3: Construção do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K"
      },
      "source": [
        "## Camada Squad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvNOk-g4odq"
      },
      "source": [
        "class BertSquardLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(BertSquardLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(units=2,\n",
        "                                             kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)]\n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf"
      },
      "source": [
        "## Modelo completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_t4ksCC7BkT"
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "  def __init__(self, name=\"bert_squad\"):\n",
        "    super(BERTSquad, self).__init__(name=name)\n",
        "    self.bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                                     trainable = True)\n",
        "    self.squad_layer = BertSquardLayer()\n",
        "\n",
        "  def apply_bert(self, inputs):\n",
        "    _, sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                          inputs[\"input_mask\"],\n",
        "                                          inputs[\"input_type_ids\"]])\n",
        "    return sequence_output\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_outputs = self.apply_bert(inputs)\n",
        "    start_logits, end_logits = self.squad_layer(seq_outputs)\n",
        "    return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a"
      },
      "source": [
        "# Etapa 4: Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      },
      "source": [
        "## Criação da IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlMKUabY9TA8"
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 2000\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsyxrxGF-K4j"
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTCoAnr-SGH"
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZVWgUGV-XVQ"
      },
      "source": [
        "optimizer = optimization.create_optimizer(init_lr=INIT_LR,\n",
        "                                          num_train_steps=NB_BATCHES_TRAIN,\n",
        "                                          num_warmup_steps = WARMUP_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mz6_N-n-myi"
      },
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "  start_positions = labels['start_positions']\n",
        "  end_positions = labels['end_positions']\n",
        "  start_logits, end_logits = model_outputs\n",
        "\n",
        "  start_loss = tf.keras.backend.sparse_categorical_crossentropy(start_positions, start_logits, from_logits=True)\n",
        "  end_loss = tf.keras.backend.sparse_categorical_crossentropy(end_positions, end_logits, from_logits=True)\n",
        "\n",
        "  total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQUzfOQ__3Hj"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk9_8xqe_8yN"
      },
      "source": [
        "bert_squad.compile(optimizer, squad_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659553aa-89f5-404f-a6d8-9733d48ef2af"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Cursos - recursos/BERT/Q&A/\"\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest checkpoint restored!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl"
      },
      "source": [
        "## Treinamento personalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7bsKxElBmWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03df522-279f-4f19-9337-3a3dc5d6e4ee"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "  print(\"Start of epoch {}\".format(epoch + 1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "    with tf.GradientTape() as tape:\n",
        "      model_outputs = bert_squad(inputs)\n",
        "      loss = squad_loss_fn(targets, model_outputs)\n",
        "\n",
        "    gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1, batch, train_loss.result()))\n",
        "\n",
        "    if batch % 500 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 0.5710\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Cursos - recursos/BERT/Q&A/ckpt-25\n",
            "Epoch 1 Batch 50 Loss 0.9684\n",
            "Epoch 1 Batch 100 Loss 1.0702\n",
            "Epoch 1 Batch 150 Loss 1.0109\n",
            "Epoch 1 Batch 200 Loss 1.0355\n",
            "Epoch 1 Batch 250 Loss 1.0123\n",
            "Epoch 1 Batch 300 Loss 0.9947\n",
            "Epoch 1 Batch 350 Loss 0.9826\n",
            "Epoch 1 Batch 400 Loss 0.9500\n",
            "Epoch 1 Batch 450 Loss 0.9253\n",
            "Epoch 1 Batch 500 Loss 0.9003\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Cursos - recursos/BERT/Q&A/ckpt-26\n",
            "Epoch 1 Batch 550 Loss 0.8746\n",
            "Epoch 1 Batch 600 Loss 0.8666\n",
            "Epoch 1 Batch 650 Loss 0.8447\n",
            "Epoch 1 Batch 700 Loss 0.8243\n",
            "Epoch 1 Batch 750 Loss 0.8015\n",
            "Epoch 1 Batch 800 Loss 0.7919\n",
            "Epoch 1 Batch 850 Loss 0.7923\n",
            "Epoch 1 Batch 900 Loss 0.7848\n",
            "Epoch 1 Batch 950 Loss 0.7674\n",
            "Epoch 1 Batch 1000 Loss 0.7545\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Cursos - recursos/BERT/Q&A/ckpt-27\n",
            "Epoch 1 Batch 1050 Loss 0.7468\n",
            "Epoch 1 Batch 1100 Loss 0.7327\n",
            "Epoch 1 Batch 1150 Loss 0.7184\n",
            "Epoch 1 Batch 1200 Loss 0.7093\n",
            "Epoch 1 Batch 1250 Loss 0.7076\n",
            "Epoch 1 Batch 1300 Loss 0.7103\n",
            "Epoch 1 Batch 1350 Loss 0.7171\n",
            "Epoch 1 Batch 1400 Loss 0.7150\n",
            "Epoch 1 Batch 1450 Loss 0.7116\n",
            "Epoch 1 Batch 1500 Loss 0.7062\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Cursos - recursos/BERT/Q&A/ckpt-28\n",
            "Epoch 1 Batch 1550 Loss 0.7051\n",
            "Epoch 1 Batch 1600 Loss 0.7106\n",
            "Epoch 1 Batch 1650 Loss 0.7252\n",
            "Epoch 1 Batch 1700 Loss 0.7317\n",
            "Epoch 1 Batch 1750 Loss 0.7388\n",
            "Epoch 1 Batch 1800 Loss 0.7428\n",
            "Epoch 1 Batch 1850 Loss 0.7392\n",
            "Epoch 1 Batch 1900 Loss 0.7352\n",
            "Epoch 1 Batch 1950 Loss 0.7385\n",
            "Time taken for 1 epoch: 40747.248958826065 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 0.0782\n",
            "Saving checkpoint for epoch 2 at /content/drive/My Drive/Cursos - recursos/BERT/Q&A/ckpt-29\n",
            "Epoch 2 Batch 50 Loss 0.5677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t"
      },
      "source": [
        "# Etapa 5: Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C"
      },
      "source": [
        "## Preparação da avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      },
      "source": [
        "Obter o conjunto dev na sessão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD"
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/Cursos - recursos/BERT/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD"
      },
      "source": [
        "Definição da função que gerará o arquivo tf_record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o"
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/Cursos - recursos/BERT/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I"
      },
      "source": [
        "Criação do tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo"
      },
      "source": [
        "Criação da função que adicona características na lista eva_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      },
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_"
      },
      "source": [
        "Criação das características e geração do arquivo tf.record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb"
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx"
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844"
      },
      "source": [
        "Carregamento da base de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Cursos - recursos/BERT/eval.tf_record\",\n",
        "    384, # input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8"
      },
      "source": [
        "## Fazendo as previsões com as funções do Google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ANeWWPXaAp"
      },
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIciBG8lXyuD"
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "  for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                  predictions['start_logits'],\n",
        "                                                  predictions['end_logits']):\n",
        "    yield RawResult(\n",
        "        unique_id=unique_ids.numpy(),\n",
        "        start_logits=start_logits.numpy().tolist(),\n",
        "        end_logits=end_logits.numpy().tolist()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrZwxU6yYnKq"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "  x, _ = inputs\n",
        "  #print(x)\n",
        "  unique_ids = x.pop(\"unique_ids\")\n",
        "  start_logits, end_logits = bert_squad(x, training=False)\n",
        "  output_dict = dict(\n",
        "      unique_ids = unique_ids,\n",
        "      start_logits=start_logits,\n",
        "      end_logits=end_logits\n",
        "  )\n",
        "  for result in get_raw_results(output_dict):\n",
        "    all_results.append(result)\n",
        "\n",
        "  print(count)\n",
        "  if count % 100 == 0:\n",
        "    print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDIK4WseZUaK"
      },
      "source": [
        "len(all_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrdG95S-hCes"
      },
      "source": [
        "all_results[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAsehQkahsMF"
      },
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/Cursos - recursos/BERT/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/Cursos - recursos/BERT/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/Cursos - recursos/BERT/null_odds.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8g8JTAiEWQ"
      },
      "source": [
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx"
      },
      "source": [
        "## Fazendo previsões personalizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha"
      },
      "source": [
        "### Criação do dicionário de inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ym06baFpwl8"
      },
      "source": [
        "def is_whitespace(c):\n",
        "  if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfwF4Dl5qacP"
      },
      "source": [
        "is_whitespace(\"a\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79B0iuKlqfed"
      },
      "source": [
        "is_whitespace(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z275QUYJqisR"
      },
      "source": [
        "is_whitespace(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ePnuokzqvLy"
      },
      "source": [
        "def whitespace_split(text):\n",
        "  doc_tokens = []\n",
        "  prev_is_whitespace = True\n",
        "  for c in text:\n",
        "    if is_whitespace(c):\n",
        "      prev_is_whitespace = True\n",
        "    else:\n",
        "      if prev_is_whitespace:\n",
        "        doc_tokens.append(c)\n",
        "      else:\n",
        "        doc_tokens[-1] += c\n",
        "      prev_is_whitespace = False\n",
        "  return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1wiH7asreK2"
      },
      "source": [
        "whitespace_split(\"My dog likes strawberries.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2dqla0Ms_sn"
      },
      "source": [
        "tokenizer.tokenize(\"My\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbUVsOkQtEAC"
      },
      "source": [
        "t = tokenizer.tokenize(\"strawberries\")\n",
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVYE-lWHtWzv"
      },
      "source": [
        "len(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38NVIT4EtfkP"
      },
      "source": [
        "[1] * len(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCcfDrqxsC4Y"
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "  text_tok = []\n",
        "  tok_to_word_id = []\n",
        "  for word_id, word in enumerate(text_words):\n",
        "    word_tok = tokenizer.tokenize(word)\n",
        "    text_tok += word_tok\n",
        "    tok_to_word_id += [word_id] * len(word_tok)\n",
        "  return text_tok, tok_to_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hepK7DMt48e"
      },
      "source": [
        "tokenize_context(whitespace_split(\"My dog likes strawberries.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23DqfgC5uR_0"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNVSJvzsu4Wi"
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKx4jBf8u8fq"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I02c_4rrvEeI"
      },
      "source": [
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQIye3SMyDVG"
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "  question_tok = tokenizer.tokenize(my_question)\n",
        "  #print(question_tok)\n",
        "\n",
        "  context_words = whitespace_split(context)\n",
        "  #print(context_words)\n",
        "  context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "  #print(context_tok)\n",
        "  #print(context_tok_to_word_id)\n",
        "\n",
        "  input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "  #print(input_tok)\n",
        "  input_tok += [\"[PAD]\"]*(384-len(input_tok))\n",
        "  #print(input_tok)\n",
        "\n",
        "  input_dict = {}\n",
        "  input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "  input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "  input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "  return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7zdHiJ5yRcQ"
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVVE-Oid06fz"
      },
      "source": [
        "my_input_dict[\"input_word_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xffiRkBh1I1p"
      },
      "source": [
        "my_input_dict[\"input_mask\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrTZ1o5C1Rya"
      },
      "source": [
        "my_input_dict[\"input_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plsFEJSx1mCK"
      },
      "source": [
        "print(my_context_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a33Y5l1f1sW4"
      },
      "source": [
        "print(context_tok_to_word_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-F8RIhd1yD-"
      },
      "source": [
        "question_tok_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWu4skOu2EQH"
      },
      "source": [
        "### Previsões"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYBmto3u2GOD"
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15UEUa2h4sfH"
      },
      "source": [
        "start_logits[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGJbnfig5oOw"
      },
      "source": [
        "question_tok_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqonmAqk5XjA"
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len + 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcmaKuWR5zTS"
      },
      "source": [
        "start_logits_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXNF-C5b6Emw"
      },
      "source": [
        "end_logits_context = end_logits.numpy()[0, question_tok_len + 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYtZuzOd6dA9"
      },
      "source": [
        "print(context_tok_to_word_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEbgb9AX6NmY"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "start_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl5MhPkx7Bw1"
      },
      "source": [
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]\n",
        "end_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOwvsoMWbsaz"
      },
      "source": [
        "### Resposta final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRIS0202AEza"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id + 1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Gt2_lDAf1w"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "#print(marked_text)\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
